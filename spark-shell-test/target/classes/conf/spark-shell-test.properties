# spark kerberos \u914D\u7F6E
spark.kerberos.is.open=false
spark.submit.principal=spark/master@HADOOP.COM
spark.submit.keytab=yarn.keytab

# socket \u7AEF\u53E3\u914D\u7F6E
spark.socket.port=8888
spark.socket.ip=192.168.0.113

# hadoop \u6587\u4EF6\u914D\u7F6E
hadoop.conf.dir=/etc/hadoop/conf.cloudera.yarn

java.security.krb5.conf=/etc/krb5.conf
kerberos.user.name=clin@HUAXIHDP.COM
java.security.auth.login.config=/etc/security/keytabs/clin.keytab


hoodie.insert.shuffle.parallelism=2
hoodie.upsert.shuffle.parallelism=2
hoodie.datasource.write.storage.type=COPY_ON_WRITE
hoodie.datasource.write.recordkey.field=rowkey
hoodie.datasource.write.precombine.field=lastupdatedttm
hoodie.datasource.write.partitionpath.field=fk_id
hoodie.datasource.hive_sync.jdbcurl=jdbc:hive2://master:10000/xh
hoodie.datasource.hive_sync.username=hive
hoodie.datasource.hive_sync.password=hive


hoodie.import.ssh.hudi.hive.sync.host=192.168.0.112
hoodie.import.ssh.hudi.hive.sync.port=22
hoodie.import.ssh.hudi.hive.sync.user=root
hoodie.import.ssh.hudi.hive.sync.pass=zaq12wsx#
hoodie.import.ssh.hudi.hive.sync.shell.path=/home/xiahu/run_sync_tool.sh



spark.driver.extraClassPath=/etc/hive/conf
spark.driver.extraJavaOptions=-XX:+PrintTenuringDistribution -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCApplicationStoppedTime -XX:+PrintGCApplicationConcurrentTime -XX:+PrintGCTimeStamps -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/hoodie-heapdump.hprof
spark.driver.maxResultSize=2g
spark.driver.memory=4g
spark.executor.cores=1
spark.executor.id=driver
spark.executor.instances=5
spark.executor.memory=4g

spark.executor.extraJavaOptions=-XX:+PrintFlagsFinal -XX:+PrintReferenceGC -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintAdaptiveSizePolicy -XX:+UnlockDiagnosticVMOptions -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/hoodie-heapdump.hprof
spark.task.cpus=1
spark.task.maxFailures=4
spark.yarn.driver.memoryOverhead=1024
spark.yarn.executor.memoryOverhead=3072
spark.yarn.max.executor.failures=100

spark.shuffle.memoryFraction=0.2
spark.shuffle.service.enabled=true
spark.storage.memoryFraction=0.6
spark.streaming.backpressure.enabled=true
spark.streaming.kafka.maxRatePerPartition=1000

spark.submit.deployMode=client

spark.rdd.compress=true
spark.kryoserializer.buffer.max=512m
spark.serializer=org.apache.spark.serializer.KryoSerializer
spark.sql.hive.convertMetastoreParquet=false
